{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87327b03-e855-4418-83ce-f1bb7e7e564e",
   "metadata": {},
   "source": [
    "### Group ID: IR GROUP 73\n",
    "### Group Members Name with Student ID:\n",
    "1.RATNABABU MAMIDI - 2023AB05147<br>\n",
    "2.Gujja Anirudh - 2023AA05236<br>\n",
    "3.GUNTUKU ANANDA RAJ - 2023AB05189<br>\n",
    "4.S SUJANA - 2023AB05187<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4495911-f378-46f8-9fbf-ecfdf196accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: PyPDF2 in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk pandas PyPDF2 python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67865f65-a890-4c85-9831-1dcf64820f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aniru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aniru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from docx import Document as docx_document\n",
    "from PyPDF2 import PdfReader\n",
    "import openpyxl\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c24b33-55df-4f98-93e7-3399b6dc5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class SkipNode:\n",
    "    def __init__(self, value, level):\n",
    "        self.value = value\n",
    "        self.forward = [None] * (level + 1)\n",
    "\n",
    "class SkipList:\n",
    "    def __init__(self, max_level, p):\n",
    "        self.max_level = max_level\n",
    "        self.p = p\n",
    "        self.header = self.create_node(self.max_level, (-1, -1))\n",
    "        self.level = 0\n",
    "\n",
    "    def create_node(self, level, value):\n",
    "        return SkipNode(value, level)\n",
    "\n",
    "    def random_level(self):\n",
    "        level = 0\n",
    "        while random.random() < self.p and level < self.max_level:\n",
    "            level += 1\n",
    "        return level\n",
    "\n",
    "    def insert(self, value):\n",
    "        update = [None] * (self.max_level + 1)\n",
    "        current = self.header\n",
    "        for i in range(self.level, -1, -1):\n",
    "            while current.forward[i] and current.forward[i].value < value:\n",
    "                current = current.forward[i]\n",
    "            update[i] = current\n",
    "        current = current.forward[0]\n",
    "\n",
    "        if current is None or current.value != value:\n",
    "            level = self.random_level()\n",
    "            if level > self.level:\n",
    "                for i in range(self.level + 1, level + 1):\n",
    "                    update[i] = self.header\n",
    "                self.level = level\n",
    "            new_node = self.create_node(level, value)\n",
    "            for i in range(level + 1):\n",
    "                new_node.forward[i] = update[i].forward[i]\n",
    "                update[i].forward[i] = new_node\n",
    "\n",
    "    def search(self, value):\n",
    "        current = self.header\n",
    "        for i in range(self.level, -1, -1):\n",
    "            while current.forward[i] and current.forward[i].value < value:\n",
    "                current = current.forward[i]\n",
    "        current = current.forward[0]\n",
    "        if current and current.value == value:\n",
    "            return current\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef4c880-9ebb-4026-a2bb-1ca53836aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a .docx file.\n",
    "\n",
    "    :param file_path: The path to the .docx file.\n",
    "    :return: The full text of the document as a single string.\n",
    "    \"\"\"\n",
    "    doc = docx_document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a .pdf file.\n",
    "\n",
    "    :param file_path: The path to the .pdf file.\n",
    "    :return: The full text of the document as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        full_text = []\n",
    "        for page in reader.pages:\n",
    "            full_text.append(page.extract_text())\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def read_xlsx(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a .xlsx file (assuming single sheet).\n",
    "\n",
    "    :param file_path: The path to the .xlsx file.\n",
    "    :return: The full text of the spreadsheet as a single string.\n",
    "    \"\"\"\n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    sheet = workbook.active\n",
    "    full_text = []\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        for cell in row:\n",
    "            if cell:\n",
    "                full_text.append(str(cell))\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def read_txt(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from a .txt file.\n",
    "\n",
    "    :param file_path: The path to the .txt file.\n",
    "    :return: The full text of the document as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses text by tokenizing, removing stop words, and stemming.\n",
    "\n",
    "    :param text: The raw text to preprocess.\n",
    "    :return: A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def process_documents(directory):\n",
    "    \"\"\"\n",
    "    Processes all documents in a directory to construct a positional index.\n",
    "\n",
    "    :param directory: The path to the directory containing the documents.\n",
    "    :return: A positional index for all documents in the directory.\n",
    "    \"\"\"\n",
    "    positional_index = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith('.docx'):\n",
    "            text = read_docx(file_path)\n",
    "        elif filename.endswith('.pdf'):\n",
    "            text = read_pdf(file_path)\n",
    "        elif filename.endswith('.xlsx'):\n",
    "            text = read_xlsx(file_path)\n",
    "        elif filename.endswith('.txt'):\n",
    "            text = read_txt(file_path)\n",
    "        else:\n",
    "            continue\n",
    "        tokens = preprocess_text(text)\n",
    "        doc_id = filename\n",
    "        for position, token in enumerate(tokens):\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = SkipList(16, 0.5)\n",
    "            positional_index[token].insert((doc_id, position))\n",
    "    return positional_index\n",
    "\n",
    "# Specifying the directory containing the documents\n",
    "documents_directory = r'C:\\Users\\aniru\\OneDrive\\Desktop\\IR Corpus'\n",
    "\n",
    "# Processing the documents to create the positional index\n",
    "positional_index = process_documents(documents_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f224b1ef-1924-4186-b4a2-9e76217524f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average search time in basic inverted index: 0.0000007200 seconds\n",
      "Average search time in skip list-enhanced index: 0.0000010400 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "def search_inverted_index(positional_index, term):\n",
    "    \"\"\"\n",
    "    Searches for a term in the basic inverted index.\n",
    "\n",
    "    :param positional_index: The positional index.\n",
    "    :param term: The term to search for.\n",
    "    :return: The postings list for the term if found, otherwise None.\n",
    "    \"\"\"\n",
    "    if term in positional_index:\n",
    "        return positional_index[term]\n",
    "    return None\n",
    "\n",
    "def search_skip_list(positional_index, term):\n",
    "    \"\"\"\n",
    "    Searches for a term in the skip list-enhanced index.\n",
    "\n",
    "    :param positional_index: The positional index.\n",
    "    :param term: The term to search for.\n",
    "    :return: The postings list for the term if found, otherwise None.\n",
    "    \"\"\"\n",
    "    if term in positional_index:\n",
    "        skip_list = positional_index[term]\n",
    "        results = []\n",
    "        current = skip_list.header.forward[0]\n",
    "        while current:\n",
    "            if current.value[0] == term:\n",
    "                results.append(current.value)\n",
    "            current = current.forward[0]\n",
    "        return results\n",
    "    return None\n",
    "\n",
    "def compare_search_performance(queries, positional_index):\n",
    "    \"\"\"\n",
    "    Compares the search performance between the basic inverted index and the skip list-enhanced index.\n",
    "\n",
    "    :param queries: A list of query terms.\n",
    "    :param positional_index: The positional index.\n",
    "    \"\"\"\n",
    "    basic_times = []\n",
    "    skip_list_times = []\n",
    "\n",
    "    for query in queries:\n",
    "        # Measuring search time for basic inverted index\n",
    "        start = time.perf_counter()\n",
    "        search_inverted_index(positional_index, query)\n",
    "        end = time.perf_counter()\n",
    "        basic_times.append(end - start)\n",
    "\n",
    "        # Measuring search time for skip list-enhanced index\n",
    "        start = time.perf_counter()\n",
    "        search_skip_list(positional_index, query)\n",
    "        end = time.perf_counter()\n",
    "        skip_list_times.append(end - start)\n",
    "\n",
    "    # Calculating average search times\n",
    "    avg_basic_time = sum(basic_times) / len(basic_times)\n",
    "    avg_skip_list_time = sum(skip_list_times) / len(skip_list_times)\n",
    "\n",
    "    # Printing average search times\n",
    "    print(f\"Average search time in basic inverted index: {avg_basic_time:.10f} seconds\")\n",
    "    print(f\"Average search time in skip list-enhanced index: {avg_skip_list_time:.10f} seconds\")\n",
    "\n",
    "# Sample queries for performance comparison\n",
    "queries = ['independence', 'tackled', 'clay', 'Chola', 'beads']\n",
    "\n",
    "# Comparing search performance\n",
    "compare_search_performance(queries, positional_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e05d6c5-1715-40e4-ac5c-d48f754c5a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wildcard query results: ['throne', 'trade', 'tree', 'theme', 'trace', 'take', 'time', 'thrive', 'te', 'tale']\n"
     ]
    }
   ],
   "source": [
    "def wildcard_query(positional_index, start_char, end_char):\n",
    "    \"\"\"\n",
    "    Performs a wildcard query on the positional index to find terms that start with a specific character and end with another specific character.\n",
    "\n",
    "    :param positional_index: The positional index containing terms and their postings lists.\n",
    "    :param start_char: The character that the terms should start with.\n",
    "    :param end_char: The character that the terms should end with.\n",
    "    :return: A list of terms that match the wildcard query.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for term in positional_index:\n",
    "        if term.startswith(start_char) and term.endswith(end_char):\n",
    "            results.append(term)\n",
    "    return results\n",
    "\n",
    "# Performing a wildcard query for terms that start with 't' and end with 'e'\n",
    "wildcard_results = wildcard_query(positional_index, 't', 'e')\n",
    "print(f\"Wildcard query results: {wildcard_results}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
